{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#原始数据集\n",
    "#数据来源：http://jmcauley.ucsd.edu/data/amazon/\n",
    "#与UCSD下载的数据集有些出入\n",
    "#JSON格式\n",
    "\n",
    "#课上使用的数据集\n",
    "#未去除特殊符号，不影响本project\n",
    "#仅提取reviewertext和overall两列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wing/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import functools\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pipeline每个步骤都有相应的package\n",
    "#可以尝试不同的方法，或方法组合\n",
    "\n",
    "#Vectorization\n",
    "#BoW，磁带方法, Bag of Words\n",
    "#N-Gram，举例N=2\n",
    "#Word2Vec，自己尝试，效果比BoW好\n",
    "\n",
    "#Model\n",
    "#Max Entropy，本质上是logistic regression\n",
    "#SVM,经典模型\n",
    "#LSTM,long shortern memory,长短记忆法，难一点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Familiar with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file into DataFrame\n",
    "kindle_data = pd.read_csv('sampled_data.csv')\n",
    "type(kindle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall    : pos\n",
      "reviewText : This book ended even before it started and it made me want for more. Oh oh such a teaser. I want the book now please. So exciting.\n"
     ]
    }
   ],
   "source": [
    "# Print first row\n",
    "# Format: data_frame.col_nam[row]\n",
    "print(\"overall    :\", kindle_data.overall[0])\n",
    "print(\"reviewText :\", kindle_data.reviewText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126871"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of kindle_data\n",
    "len(kindle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>This book ended even before it started and it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>This is a great read with so much emotion you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>It&amp;#8217;s Christmas Eve and miraculously, Sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>I enjoyed meeting the character of Cassandra. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Can I be the next Hunter wife?  Again, I have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  overall                                         reviewText\n",
       "0     pos  This book ended even before it started and it ...\n",
       "1     pos  This is a great read with so much emotion you ...\n",
       "2     pos  It&#8217;s Christmas Eve and miraculously, Sal...\n",
       "3     pos  I enjoyed meeting the character of Cassandra. ...\n",
       "4     pos  Can I be the next Hunter wife?  Again, I have ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a sample (head) of the data frame\n",
    "kindle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    64559\n",
       "neg    62312\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statics on tags\n",
    "kindle_data.overall.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/10min.html?highlight=data%20frame) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split complete data set into [pos, neg]\n",
    "def splitPosNeg(data_):\n",
    "    neg = data_.loc[data_.overall=='neg']\n",
    "    pos = data_.loc[data_.overall=='pos']\n",
    "    return [pos,neg]\n",
    "\n",
    "[pos,neg] = splitPosNeg(kindle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "pos: 64559 , neg: 62312\n"
     ]
    }
   ],
   "source": [
    "print(type(pos))\n",
    "print(\"pos:\", len(pos), \", neg:\", len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#大型数据处理，debug很麻烦，两种简单的处理方法：\n",
    "#1.逐步验证\n",
    "#  图像处理，验证结果矩阵\n",
    "#2.先用小数据集验证程序是否正确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1、去掉无意义的标点符号\n",
    "#2、去掉对情感极性分析无实际意义的词，stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "**********************************************************************\n",
    "  Resource 'corpora/stopwords' not found.  Please use the NLTK\n",
    "  Downloader to obtain the resource:  >>> nltk.download()\n",
    "  Searched in:\n",
    "    - '/home/wing/nltk_data'\n",
    "    - '/usr/share/nltk_data'\n",
    "    - '/usr/local/share/nltk_data'\n",
    "    - '/usr/lib/nltk_data'\n",
    "    - '/usr/local/lib/nltk_data'\n",
    "**********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义没有实际意义的词，此处使用nltk中默认的英文中的stopwords\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#定义翻译表，将标点符号替换为长度相同的空格\n",
    "translation = str.maketrans(string.punctuation,' '*len(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dedef'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#翻译表使用示例\n",
    "transtbl = str.maketrans('abc','def')\n",
    "'ababc'.translate(transtbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将每一条review转化为tokens\n",
    "#\n",
    "#\n",
    "def preprocessing(line):\n",
    "    tokens=[]\n",
    "    line = str(line).translate(translation)  # Replace punctuation\n",
    "    line = nltk.word_tokenize(line.lower())  # Tokenization，传入转化为小写的line\n",
    "    \n",
    "    for t in line:\n",
    "        # Remove stopwords\n",
    "        if t not in stop:\n",
    "            stemmed = lemmatizer.lemmatize(t)#词形还原\n",
    "            tokens.append(stemmed)\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yet a more compact way to write the code\n",
    "def preprocessing(line: str) -> str:\n",
    "    #line: str, python3新增的定义方法，指定输入变量的类型，有一个强制检查，输入不是str会有warning\n",
    "    #-> str, python3新增标记方法，指定输出的类型\n",
    "    line = str(line).translate(translation)\n",
    "    line = nltk.word_tokenize(line.lower())\n",
    "    \n",
    "    line = [lemmatizer.lemmatize(t) for t in line if t not in stop]\n",
    "    #list comprehension, python中的语法，类似于sql\n",
    "    #lemmatizer.lemmatize(t), 相当于select子句\n",
    "    #for，t的来源，相当于from子句\n",
    "    #if，t需满足的条件，相当于where子句\n",
    "    return ' '.join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought yesterday really love'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"I bought it yesterday and I really love it!\"\n",
    "preprocessing(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#注意：此处bought没有被还原\n",
    "#因lemmatizer.lemmatize（）默认只对名词进行还原\n",
    "#若需对动词还原，需设置lemmatizer.lemmatize(word, pos='n')，pos='v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对动词也进行还原的预处理\n",
    "def preprocessing2(line: str) -> str:\n",
    "    line = str(line).translate(translation)\n",
    "    line = nltk.word_tokenize(line.lower())\n",
    "    \n",
    "    line = [lemmatizer.lemmatize(t, pos='v') for t in line if t not in stop]\n",
    "    return ' '.join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy yesterday really love'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing2(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess all data\n",
    "pos_data = [preprocessing(p) for p in pos['reviewText']]\n",
    "neg_data = [preprocessing(p) for p in neg['reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yet a more modern way to write code\n",
    "#map()参数1，处理方法\n",
    "#map()参数2，可迭代的数据\n",
    "pos_data = list(map(preprocessing, pos['reviewText']))\n",
    "neg_data = list(map(preprocessing, neg['reviewText']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some modern functions to introduce\n",
    "- map\n",
    "- reduce\n",
    "- filter\n",
    "\n",
    "They are very useful when running the project on a cluster or distributed compute system like Hadoop or Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#现在编程语言，如java，都加入了map和lambda\n",
    "#大数据处理平台，hadoop-->spark，将数据操作简化为3类\n",
    "#map，对数据进行同样的处理，映射\n",
    "#reduce，对应统计的方法，比如平均值，最大值等\n",
    "#filter，对数据进行筛选\n",
    "#便于平台间数据接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# Some useful modern functions\n",
    "l = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# Map\n",
    "def square(x: int) -> int:\n",
    "    return x * x\n",
    "\n",
    "print( list(map(square, l)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# Using lambda function\n",
    "print( list(map(lambda x: x * x, l)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] by add: 45\n"
     ]
    }
   ],
   "source": [
    "# Reduce\n",
    "# reduce function is moved to functools\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "rst = functools.reduce(add, l)\n",
    "print (\"reduce\", l, \"by add:\", rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] by add: 45\n"
     ]
    }
   ],
   "source": [
    "# Using lambda function\n",
    "# reduce is moved to functools in Python 3\n",
    "rst = functools.reduce(lambda x, y: x + y, l)\n",
    "print (\"reduce\", l, \"by add:\", rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] by max: 9\n"
     ]
    }
   ],
   "source": [
    "rst = functools.reduce(lambda x, y: max(x, y), l)\n",
    "print (\"reduce\", l, \"by max:\", rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter\n",
    "# Much faster than loop, similar with list comprehension\n",
    "list(filter(lambda x: x < 5, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training Data & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pos_data + neg_data\n",
    "# remember this is sampled\n",
    "labels = np.concatenate((pos['overall'].values,neg['overall'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training set and testing set (20:80)\n",
    "# stratify: make sure pos/neg remains the same in training set and testing set\n",
    "#用于需要迭代优化的算法，SVM，LogisticRegression，便于结果预估\n",
    "#朴素贝耶斯不需要，因为不涉及迭代优化过程\n",
    "train_data, test_data, train_labels, test_labels = \\\n",
    "train_test_split(\n",
    "    data, \n",
    "    labels, \n",
    "    test_size=0.2, #切分比例，test_data的占比为20%\n",
    "    stratify=labels, #随机切分，防止抽样不均衡，此处=labels，表示pos:neg与原始数据labels中比例相同\n",
    "    random_state=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size =  101496 testing size =  25375\n"
     ]
    }
   ],
   "source": [
    "print(\"training size = \", len(train_data), \"testing size = \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Underfitting vs Overfitting\n",
    "![](http://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_001.png)\n",
    "\n",
    "Common Method:\n",
    "- 20:80 Split\n",
    "- K-fold\n",
    "\n",
    "To estimate accuracy (f-score):\n",
    "- 20:20:60 Split\n",
    "- 10:20:70 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10:20:70更常用\n",
    "#10，验证\n",
    "#20，测试\n",
    "#70，训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Push all tokens and compute frequency of words\n",
    "#将所有tokens展开放入一维数组\n",
    "t = []\n",
    "for line in train_data:\n",
    "    l = nltk.word_tokenize(line)\n",
    "    for w in l:\n",
    "        t.append(w)\n",
    "\n",
    "#统计频数        \n",
    "word_features = nltk.FreqDist(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yet another more python-y style\n",
    "tokens = [word for line in train_data \\\n",
    "               for word in nltk.word_tokenize(line)]\n",
    "\n",
    "word_features = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 84617 samples and 4195256 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 124073),\n",
       " ('story', 62132),\n",
       " ('read', 55777),\n",
       " ('one', 36880),\n",
       " ('like', 32032),\n",
       " ('character', 30265),\n",
       " ('good', 28930),\n",
       " ('would', 27660),\n",
       " ('author', 24824),\n",
       " ('love', 24805)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#认为限制前10000个词\n",
    "topwords = [fpair[0] for fpair in list(word_features.most_common(10000))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "cnt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ngram_range=(1, 1),BAG of words,默认\n",
    "#ngram_range=(2, 2),bi-gram\n",
    "#ngram_range=(1, 2),(Uni+Bi)-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x9969 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9969 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our BAG of words (specify words we care about)\n",
    "#一维矩阵，即单词表， bag\n",
    "cnt_fit = cnt_vec.fit_transform([' '.join(topwords)])\n",
    "cnt_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf–idf term weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tf: term-frequency\n",
    "- idf: inverse document-frequency\n",
    "- Tf-idf = $tf(t,d) \\times idf(t)$\n",
    "\n",
    "$$\n",
    "idf(t) = log{\\frac{1 + nd}{1 + df(d, t)}} + 1\n",
    "$$\n",
    "\n",
    "![](http://www.onemathematicalcat.org/Math/Algebra_II_obj/Graphics/log_base_gt1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sentent 1: The boy **love** the toy\n",
    "\n",
    "> Sentent 2: The boy **hate** the toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#并非出现次数越多，对分类结果影响越大，比如toy，没有倾向性\n",
    "#love仅在pos出现，对结果的区分度更大\n",
    "#而naive bayes中，出现次数越多的词，对结果的影响越大\n",
    "\n",
    "#引入调整权重的方法， Tf–idf term weighting\n",
    "#公式中，nd是document数量，df(d,t)是document frequency\n",
    "#在上面的例子中，\n",
    "#love出现1次，df(d,t)=1，nd=2（两个句子），其idf=log(1+2)/(1+1)+1>1\n",
    "#boy,其idf=log(1+2)/(1+2)+1=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False,\n",
       "         use_idf=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes，独立性假设\n",
    "transformer = TfidfTransformer(smooth_idf=False)#smooth_idf，平滑操作，防止违反独立性假设出现0，导致连乘的计算结果为0的情况\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature1，每行出现多次，说明没什么意义\n",
    "#feature3、feature3，仅在不同行出现，对分类结果比较有意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81940995,  0.        ,  0.57320793],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.47330339,  0.88089948,  0.        ],\n",
       "       [ 0.58149261,  0.        ,  0.81355169]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x9969 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9969 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_trans = TfidfTransformer()\n",
    "tf_fit = tf_trans.fit_transform(cnt_fit)\n",
    "tf_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x9969 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9969 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since CountVectorizer and TfidTransformer are often used together，可用于替代之前的两个步骤\n",
    "# There is a class named TfidfVectorizer that combine these two steps\n",
    "tf_vec = TfidfVectorizer()\n",
    "tf_fit = tf_vec.fit_transform([' '.join(topwords)])\n",
    "tf_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据的使用\n",
    "#1.train_data-->tokens-->word_features-->topwords-->cnt_fit-->tf_fit，得到单词表；tf_vec，得到向量器\n",
    "#2.tf_vec(train_data)-->train_features-->mnb.model-->mnb.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features from training set\n",
    "# Vocabulary is from topwords\n",
    "train_features = tf_vec.transform(train_data)\n",
    "\n",
    "# cnt_train_features = cnt_vec.transform(train_data)\n",
    "# train_features = tf_trans.transform(cnt_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101496, 9969)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array[n_train_data * n_features]\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#列数9969即单词表的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features from test set\n",
    "test_features = tf_vec.transform(test_data)\n",
    "\n",
    "# cnt_test_features = cnt_vec.transform(test_data)\n",
    "# test_features = tf_trans.transform(cnt_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Multinomial NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for **classification with discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#实际使用发现，tf-idf比wordcounts效果要好\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "mnb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alpha,Laplace拉普拉斯平滑因子， (0 for no smoothing)，若单词从未出现过则假设出现了1次\n",
    "#class_prior\n",
    "#fit_prior，是否要事先学习train与test的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB model trained in 0.270455 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "start = time.time()\n",
    "mnb_model.fit(train_features, train_labels)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Multinomial NB model trained in %f seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#naive bayes的一个极大优势，非常快，几乎没有训练时间\n",
    "#SVM，特别慢\n",
    "#linear SVM，会快一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos' 'pos' ..., 'neg' 'pos' 'neg']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = mnb_model.predict(test_features)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8199408867\n"
     ]
    }
   ],
   "source": [
    "# Metrics，统计模型参数的包\n",
    "# metrics.accuracy_score(y_true, y_pred)，参数顺序颠倒亦可\n",
    "accuracy = metrics.accuracy_score(pred,test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.80      0.81     12463\n",
      "        pos       0.81      0.84      0.83     12912\n",
      "\n",
      "avg / total       0.82      0.82      0.82     25375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use keyword arguments to set arguments explicitly\n",
    "print(metrics.classification_report(y_true=test_labels, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#precision vs. accuracy\n",
    "#一般用平均的f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.50      1.00      0.67         1\n",
      "    class 1       0.00      0.00      0.00         1\n",
      "    class 2       1.00      0.67      0.80         3\n",
      "\n",
      "avg / total       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example from sklearn documentation\n",
    "#三分类问题\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(metrics.classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict a new sentence\n",
    "# vectorizer needs to be pre-fitted\n",
    "# At the end of the project, the function signature should be something like:\n",
    "# predict_new(sentent: str, vec, model) -> str\n",
    "\n",
    "def predict_new(sentence: str):\n",
    "    sentence = preprocessing(sentence)\n",
    "    features = tf_vec.transform([sentence])\n",
    "    pred = mnb_model.predict(features)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_new(\"Not bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_new(\"This product is bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这种情况怎么处理？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#？模型\n",
    "# Save vectorizer\n",
    "with open('tf_vec.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(tf_vec, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#naive bayes模型\n",
    "with open('mnb_model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(mnb_model, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & test using Uni-Gram + Bi-Gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101496, 19937)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Uni+Bi)-Gram\n",
    "bg_tf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "bg_tf_vec.fit([' '.join(topwords)])\n",
    "bg_train_features = bg_tf_vec.transform(train_data)\n",
    "\n",
    "bg_train_features.shape\n",
    "\n",
    "# Array[n_train_data * (uni_gram_features + bi_gram_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bg_test_features = bg_tf_vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract (uni+bi)-gram test featuresransform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos' 'pos' ..., 'neg' 'pos' 'neg']\n"
     ]
    }
   ],
   "source": [
    "# Train & test using (uni+bi)-gram features\n",
    "bg_mnb_model = MultinomialNB()\n",
    "bg_mnb_model.fit(bg_train_features, train_labels)\n",
    "bg_pred = bg_mnb_model.predict(bg_test_features)\n",
    "print(bg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819783251232\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "bg_accuracy = metrics.accuracy_score(bg_pred,test_labels)\n",
    "print(bg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.80      0.81     12463\n",
      "        pos       0.81      0.84      0.83     12912\n",
      "\n",
      "avg / total       0.82      0.82      0.82     25375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true=test_labels, y_pred=bg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#爬数据，scrapy.org\n",
    "\n",
    "#数据来源：\n",
    "#1、darksky.net/dev\n",
    "#2、学校官网，如UCSD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
